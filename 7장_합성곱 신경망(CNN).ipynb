{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>『 밑바닥부터 시작하는 딥러닝 』</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://image.kyobobook.co.kr/images/book/large/636/l9788968484636.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7장. 합성곱 신경망(CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#전체-구조\" data-toc-modified-id=\"전체-구조-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>전체 구조</a></span></li><li><span><a href=\"#합성곱-계층\" data-toc-modified-id=\"합성곱-계층-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>합성곱 계층</a></span><ul class=\"toc-item\"><li><span><a href=\"#완전연결-계층의-문제점\" data-toc-modified-id=\"완전연결-계층의-문제점-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>완전연결 계층의 문제점</a></span></li><li><span><a href=\"#합성곱-연산\" data-toc-modified-id=\"합성곱-연산-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>합성곱 연산</a></span></li><li><span><a href=\"#패딩\" data-toc-modified-id=\"패딩-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>패딩</a></span></li><li><span><a href=\"#스트라이드\" data-toc-modified-id=\"스트라이드-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>스트라이드</a></span></li><li><span><a href=\"#3차원-데이터의-합성곱-연산\" data-toc-modified-id=\"3차원-데이터의-합성곱-연산-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>3차원 데이터의 합성곱 연산</a></span></li><li><span><a href=\"#블록으로-생각하기\" data-toc-modified-id=\"블록으로-생각하기-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>블록으로 생각하기</a></span></li><li><span><a href=\"#배치-처리\" data-toc-modified-id=\"배치-처리-2.7\"><span class=\"toc-item-num\">2.7&nbsp;&nbsp;</span>배치 처리</a></span></li></ul></li><li><span><a href=\"#풀링-계층\" data-toc-modified-id=\"풀링-계층-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>풀링 계층</a></span></li><li><span><a href=\"#합성곱/풀링-계층-구현하기\" data-toc-modified-id=\"합성곱/풀링-계층-구현하기-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>합성곱/풀링 계층 구현하기</a></span><ul class=\"toc-item\"><li><span><a href=\"#4차원-배열\" data-toc-modified-id=\"4차원-배열-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>4차원 배열</a></span></li><li><span><a href=\"#im2col로-데이터-전개하기\" data-toc-modified-id=\"im2col로-데이터-전개하기-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>im2col로 데이터 전개하기</a></span></li><li><span><a href=\"#합성곱-계층-구현하기\" data-toc-modified-id=\"합성곱-계층-구현하기-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>합성곱 계층 구현하기</a></span></li><li><span><a href=\"#풀링-계층-구현하기\" data-toc-modified-id=\"풀링-계층-구현하기-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>풀링 계층 구현하기</a></span></li></ul></li><li><span><a href=\"#CNN-구현하기\" data-toc-modified-id=\"CNN-구현하기-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>CNN 구현하기</a></span></li><li><span><a href=\"#CNN-시각화하기\" data-toc-modified-id=\"CNN-시각화하기-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>CNN 시각화하기</a></span><ul class=\"toc-item\"><li><span><a href=\"#1번째-층의-가중치-시각화하기\" data-toc-modified-id=\"1번째-층의-가중치-시각화하기-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>1번째 층의 가중치 시각화하기</a></span></li><li><span><a href=\"#층-깊이에-따른-추출-정보-변화\" data-toc-modified-id=\"층-깊이에-따른-추출-정보-변화-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>층 깊이에 따른 추출 정보 변화</a></span></li></ul></li><li><span><a href=\"#대표적인-CNN\" data-toc-modified-id=\"대표적인-CNN-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>대표적인 CNN</a></span><ul class=\"toc-item\"><li><span><a href=\"#LeNet\" data-toc-modified-id=\"LeNet-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>LeNet</a></span></li><li><span><a href=\"#AlexNet\" data-toc-modified-id=\"AlexNet-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>AlexNet</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번 장의 주제는 **합성곱 신경망(convolutional neural network, CNN)**입니다. CNN은 이미지 인식과 음성 인식 등 다양한 곳에서 사용되는데, 특히 이미지 인식 분야에서 딥러닝을 활용한 기법은 거의 다 CNN을 기초로 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전체 구조"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번 절에서는 계층들을 어떻게 조합하여 CNN을 만드는지를 먼저 보겠습니다.  \n",
    "\n",
    "지금까지 본 신경망은 인접하는 계층의 모든 뉴런과 결합되어 있습니다. 이를 **완전 연결(fully-connected, 전결합)**이라고 하며, 완전히 연결된 계층을 **Affine 계층**이라는 이름으로 구현했습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "완전연결 신경망은 Affine 계층 뒤에 활성화 함수를 갖는 ReLU 계층(혹은 sigmoid)이 이어집니다. 이 그림에서는 **Affine-ReLU 조합이 4개**가 쌓였고, 마지막 5번째 층은 Affine 계층에 이어 **소프트맥스 계층에서 최종 결과(확률)를 출력**합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://blog.kakaocdn.net/dn/bpyCtu/btqP4lOOXW0/exKplA2DqPY6487p8UwkQ0/img.png\" width=75%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그럼 CNN의 구조는 어떻게 다를까요?  \n",
    "\n",
    "CNN에서는새로운 **합성곱 계층(Conv)**과 **풀링 계층(Pooling)**이 추가됩니다. CNN의 계층은 **Conv-ReLU-Pooling** 흐름으로 연결됩니다.  \n",
    "주목할 점은 **1) 출력에 가까운 층에서는 지금까지의 Affine-ReLU 구성을 사용할 수 있다는 것**과, **2) 마지막 출력 계층에서는 Affine-Softmax 조합을 그대로 사용한다는 것**입니다. 이상 일반적인 CNN에서 흔히 볼 수 있는 구성이었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://media.vlpt.us/post-images/dscwinterstudy/118fa610-4199-11ea-9e80-2dba9507cf00/fig-7-2.png\" width=70%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 합성곱 계층"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN에서는 **패딩(padding)**, **스트라이드(stride)** 등 CNN 고유의 용어가 등장합니다. 또, **각 계층 사이에는 3차원 데이터같이 입체적인 데이터가 흐른다는 점**에서 완전연결 신경망과 다릅니다. 이번 절에서는 CNN에서 사용하는 합성곱 계층의 구조를 차분히 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 완전연결 계층의 문제점"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지금까지 본 완전연결 신경망에서는 완전연결 계층(Affine)을 사용했습니다. 완전연결 계층에서는 인접하는 계층의 뉴런이 모두 연결되고 출력의 수는 임의로 정할 수 있습니다.  \n",
    "완전연결 계층의 문제점은 **데이터의 형상이 무시된다는 사실**입니다. 입력 데이터가 이미지인 경우를 예로 들면, 이미지는 3차원 데이터(세로, 가로, 채널(색상))이지만 완전연결 계층에 입력할 때에는 평평한 1차원 데이터로 평탄화해줘야 합니다. 이미지 형상에는 소중한 공간적 정보가 담겨있는데 완전연결 계층은 이 형상을 무시하고 모든 입력 데이터를 동등한 뉴런(같은 차원의 뉴런)으로 취급하여 형상에 담긴 정보를 살릴 수 업습니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한편, 합성곱 계층은 형상을 유지합니다. 이미지도 3차원 데이터로 입력받으며, 마찬가지로 다음 계층에도 3차원 데이터로 전달합니다. 그래서 CNN에서는 이미지처럼 형상을 가진 데이터를 제대로 이해할 (가능성이 있는) 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**< 용어 정리 >**  \n",
    "- 특징 맵(feature map): 합성곱 계층의 입출력 데이터\n",
    "- 입력 특징 맵(input feature map): 합성곱 계층의 입력 데이터\n",
    "- 출력 특징 맵(output feature map): 합성곱 계층의 출력 데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 합성곱 연산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "합성곱 계층에서의 **합성곱 연산**을 처리합니다. 합성곱 연산은 이미지 처리에서 말하는 **필터 연산**에 해당합니다.  \n",
    "합성곱 연산은 입력 데이터에 필터를 적용합니다. 문헌에 따라 필터를 커널이라 칭하기도 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://img1.daumcdn.net/thumb/R720x0.q80/?scode=mtistory2&fname=http%3A%2F%2Fcfile8.uf.tistory.com%2Fimage%2F993804415AAD2FD51461F6\" width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음은 합성곱 연산 순서를 그려본 것입니다. 합성곱 연산은 필터으 **윈도우(window)**를 일정 간격으로 이동해가며 입력 데이터에 적용합니다. 이 그림에서 보듯 입력과 필터에서 대응하는 원소끼리 곱한 후 그 총합을 구합니다. 이 계산을 **단일 곱셈-누산(fused multiply-add, FMA)**라고 합니다. 그리고 그 결과를 출력의 해당 장소에 저장합니다. 이 과정을 모든 장소에서 수행하면 합성곱 연산의 출력이 완성됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://media.vlpt.us/post-images/dscwinterstudy/1d4ed9d0-4199-11ea-8bd7-d9eb4a57ded4/fig-7-4.png\" width=40%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "완전연결 신경망에는 가중치 매개변수와 편향이 존재하는데, CNN에서는 필터의 매개변수가 그동안의 가중치에 해당합니다. 그리고 CNN에도 편향이 존재합니다. 편향은 필터를 적용한 후의 데이터에 더해집니다. 그리고 편향은 항상 하나(1x1)만 존재합니다. 그 하나의 값을 필터를 적용한 모든 원소에 더하는 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://t1.daumcdn.net/cfile/tistory/998095395AAD318E2F\" width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 패딩"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "합성곱 연산을 수행하기 전에 입력 데이터 주변을 특정 값(예컨대 0)으로 채우기도 합니다. 이를 **패딩(padding)**이라 하며, 합성곱 연산에서 자주 이용하는 기법입니다. 처음에 크기가 (4, 4)인 입력 데이터에 패딩이 추가되어 (6, 6)이 됩니다. 이 예에서는 패딩을 1로 설정했지만, 2나 3 등 원하는 정수로 설정할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://t1.daumcdn.net/cfile/tistory/99F52B4F5AAD328114\" width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "패딩은 주로 **출력 크기를 조정할 목적**으로 사용합니다. 예를 들어 (4, 4) 입력 데이터에 (3, 3) 필터를 적용하면 출력은 (2, 2)가 되어, 입력보다 2만큼 줄어듭니다. 이는 합성곱 연산을 몇 번이나 되풀이하는 심층 신경망에서는 문제가 될 수 있습니다. **합성곱 연산을 거칠 때마다 크기가 작아지면 어느 시점에서는 출력 크기가 1이 되어버려 더 이상은 합성 곱 연산을 적용할 수 없다**는 뜻입니다. 이러한 사태를 막기 위해 패딩을 사용합니다. 한 마디로 **입력 데이터의 공간적 크기를 고정한 채로 다음 계층에 전달할 수 있습니다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 스트라이드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "필터를 적용하는 위치의 간격을 **스트라이드(stride)**라고 합니다. 지금까지 본 예는 모두 스트라이드가 1이었지만, 스트라이드를 2로 하면 필터를 적용하는 윈도우가 두 칸씩 이동합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://media.vlpt.us/post-images/dscwinterstudy/3084e940-4199-11ea-b8c9-7f7f97d846d5/fig-7-7.png\" width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "스트라이드를 2로 하니 출력은 (3, 3)이 됩니다. 이처럼 스트라이드를 키우면 출력 크기는 작아집니다. 한편, 패딩을 크게 하면 출력 크기가 켜졌습니다. 이러한 관계를 수식화하면 어떻게 될까요? 이어서 패딩, 스트라이드, 출력 크기를 어떻게 계산하는지 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 입력 크기: (H, W)\n",
    "- 필터 크기: (FH, FW)\n",
    "- 출력 크기: (OH, OW)\n",
    "- 패딩: P\n",
    "- 스트라이드: S  \n",
    "\n",
    "라고 할 때 출력 크기는 다음과 같이 계산합니다.\n",
    "<img src=\"https://blog.kakaocdn.net/dn/bBCpiz/btqCDjQLelc/CiXh59A2mgVer9M2UzZW21/img.png\" width=30%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단, **출력 값은 정수로 나눠떨어지는 값이어야 한다는 점에 주의**해야 합니다. 출력 크기가 정수가 아니면 오류를 내는 등의 대응을 해줘야 합니다. 딥러닝 프레임워크 중에는 값이 딱 나눠떨어지지 않을 때는 가장 가까운 정수로 반올림하는 등, 특별히 에러를 내지 않고 진행하도록 구현하는 경우도 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3차원 데이터의 합성곱 연산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지금까지 2차원 형상을 다루는 합성곱 연산을 살펴봤습니다. 그러나 이미지만 해도 세로/가로에 더해서 채널까지 고려한 **3차원 데이터**입니다. 이번 절에서는 조금 전과 같은 순서로, 채널까지 고려한 3차원 데이터를 다루는 합성곱 연산을 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2차원일 때와 비교하면, 길이 방향(채널 방향)으로 특징 맵이 늘어났습니다. 채널쪽에서 특징 맵이 여러 개 있다면 입력 데이터와 필터의 합성곱 연산을 채널마다 수행하고, 그 결과를 더해서 하나의 출력을 얻습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://media.vlpt.us/post-images/dscwinterstudy/4d744a50-4199-11ea-8bd7-d9eb4a57ded4/fig-7-9.png\" width=40%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3차원의 합성곱 연산에서 주의할 점은 **입력 데이터의 채널 수와 필터의 채널 수가 같아야 한다는 것**입니다. 위의 예에서는 모두 3개로 일치합니다. 한편, 필터 자체의 크기는 원하는 값으로 설정할 수 있습니다. 단, **모든 채널의 필터가 같은 크기**여야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 블록으로 생각하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3차원의 합성곱 연산은 데이터와 필터를 **직육면체 블록**이라고 생각하면 쉽습니다. 데이터 형상은 (채널 수 C, 높이 H, 너비 W)로 씁니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "출력 데이터는 한 장의 특징 맵입니다. 한 장의 특징 맵을 다른 말로 하면 채널이 1개인 특징 맵입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://img1.daumcdn.net/thumb/R720x0.q80/?scode=mtistory2&fname=http%3A%2F%2Fcfile3.uf.tistory.com%2Fimage%2F998CE7355BC97F632E38E8\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그럼 합성곱 연산의 출력으로 다수의 채널을 내보내려면 어떻게 해야 할까요? 그 답은 필터(가중치)를 다수 사용하는 것입니다. 아래 그림과 같이 필터를 FN개 적용하면 출력 맵도 FN개가 생성됩니다. 이 완성된 블록을 다음 계층으로 넘기겠다는 것이 CNN의 처리 흐름입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=http%3A%2F%2Fcfile22.uf.tistory.com%2Fimage%2F99CDF2395BC97F7C2D416E\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이상에서 보듯 합성곱 연산에서는 필터의 수도 고려해야 합니다. 그런 이유로 필터의 가중치 데이터는 4차원 데이터입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 배치 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망 처리에서는 입력 데이터를 한 덩어리로 묶어 배치로 처리했습니다. 완전연결 신경망을 구현하면서는 이 방식을 지원하여 처리 효율을 높이고, 미니배치 방식의 학습도 지원하도록 했습니다.  \n",
    "\n",
    "합성곱 연산도 마찬가지로 배치 처리를 지원하고자 합니다. 그래서 각 계층을 흐르는 데이터의 차원을 하나 늘려 4차원 데이터로 저장합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://blog.kakaocdn.net/dn/c9AhzF/btqJ91URvwj/N6C3OueXq8ee8x1c96OwU1/img.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망에 4차원 데이터가 하나 흐를 때마다 데이터 N개에 대한 합성곱 연산이 이뤄진다는 것입니다. 즉, N회 분의 처리를 한 번에 수행하는 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 풀링 계층"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**풀링**은 세로와 가로 방향의 공간을 줄이는 연산입니다. 아래 그림과 같이 2x2 영역을 원소 하나로 집약하여 공간 크기를 줄입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://media.vlpt.us/post-images/dscwinterstudy/590eb800-4199-11ea-8bd7-d9eb4a57ded4/fig-7-14.png\" width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**최대 풀링(max pooling)**은 최댓값을 구하는 연산입니다. 최대 풀링 외에도 **평균 풀링(average pooling)** 등이 있습니다. 이미지 인식 분야에서는 주로 최대 풀링을 사용합니다. 참고로, 풀링의 윈도우 크기와 스트라이드는 같은 값으로 설정하는 것이 보통입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**< 풀링 계층의 특징>**  \n",
    "1. 학습해야 할 매개변수가 없다.\n",
    "2. 채널 수가 변하지 않는다.\n",
    "3. 입력의 변화에 영향을 적게 받는다.(강건하다)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 합성곱/풀링 계층 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번 절에서는 **합성곱 계층**과 **풀링 계층**을 구현해보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4차원 배열"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞에서 설명한 대로 CNN에서 계층 사이를 흐르는 데이터는 4차원입니다. 예를 들어 데이터의 형상이 (10, 1, 28, 28)이라면, 이는 높이 28, 너비 28, 채널 1개인 데이터가 10개라는 말입니다.  \n",
    "\n",
    "이처럼 CNN은 4차원 데이터를 다루기 때문에 합성곱 연산의 구현이 복잡해질 것 같지만, 다음 절에서 설명하는 **im2col**이라는 트릭이 문제를 단순하게 만들어줍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### im2col로 데이터 전개하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "합성곱 연산을 곧이곧대로 구현하려면 for문을 겹겹이 써야합니다. for문 대신 **im2col**이라는 편의 함수를 사용해 간단하게 구현해보겠습니다.  \n",
    "im2col은 **입력 데이터를 필터링(가중치 계산)하기 좋게 전대하는(펼치는) 함수**입니다. 아래 그림과 같이 3차원 입력 데이터에 im2col을 적용하면 2차원 행렬로 바뀝니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://media.vlpt.us/post-images/dscwinterstudy/7f035cf0-4199-11ea-9e80-2dba9507cf00/fig-7-17.png\" width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "따라서 필터 적용 영역을 앞에서부터 순서대로 1줄로 펼치는 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 합성곱 계층 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T04:56:39.296448Z",
     "start_time": "2021-02-19T04:56:39.292459Z"
    }
   },
   "outputs": [],
   "source": [
    "# p.245 ~ 247"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 풀링 계층 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T04:56:39.506905Z",
     "start_time": "2021-02-19T04:56:39.503910Z"
    }
   },
   "outputs": [],
   "source": [
    "# p.247 ~ 250"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T04:56:39.724323Z",
     "start_time": "2021-02-19T04:56:39.712354Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport sys, os\\nsys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\\nimport pickle\\nimport numpy as np\\nfrom collections import OrderedDict\\nfrom common.layers import *\\nfrom common.gradient import numerical_gradient\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-19T04:56:39.766210Z",
     "start_time": "2021-02-19T04:56:39.727313Z"
    }
   },
   "outputs": [],
   "source": [
    "class SimpleConvNet:\n",
    "    \"\"\"단순한 합성곱 신경망\n",
    "    \n",
    "    conv - relu - pool - affine - relu - affine - softmax\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_size : 입력 크기（MNIST의 경우엔 784）\n",
    "    hidden_size_list : 각 은닉층의 뉴런 수를 담은 리스트（e.g. [100, 100, 100]）\n",
    "    output_size : 출력 크기（MNIST의 경우엔 10）\n",
    "    activation : 활성화 함수 - 'relu' 혹은 'sigmoid'\n",
    "    weight_init_std : 가중치의 표준편차 지정（e.g. 0.01）\n",
    "        'relu'나 'he'로 지정하면 'He 초깃값'으로 설정\n",
    "        'sigmoid'나 'xavier'로 지정하면 'Xavier 초깃값'으로 설정\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=(1, 28, 28), \n",
    "                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
    "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
    "\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * \\\n",
    "                            np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * \\\n",
    "                            np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * \\\n",
    "                            np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "\n",
    "        # 계층 생성\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n",
    "                                           conv_param['stride'], conv_param['pad'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        \"\"\"손실 함수를 구한다.\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 입력 데이터\n",
    "        t : 정답 레이블\n",
    "        \"\"\"\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        acc = 0.0\n",
    "        \n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt) \n",
    "        \n",
    "        return acc / x.shape[0]\n",
    "\n",
    "    def numerical_gradient(self, x, t):\n",
    "        \"\"\"기울기를 구한다（수치미분）.\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 입력 데이터\n",
    "        t : 정답 레이블\n",
    "        Returns\n",
    "        -------\n",
    "        각 층의 기울기를 담은 사전(dictionary) 변수\n",
    "            grads['W1']、grads['W2']、... 각 층의 가중치\n",
    "            grads['b1']、grads['b2']、... 각 층의 편향\n",
    "        \"\"\"\n",
    "        loss_w = lambda w: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        for idx in (1, 2, 3):\n",
    "            grads['W' + str(idx)] = numerical_gradient(loss_w, self.params['W' + str(idx)])\n",
    "            grads['b' + str(idx)] = numerical_gradient(loss_w, self.params['b' + str(idx)])\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        \"\"\"기울기를 구한다(오차역전파법).\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 입력 데이터\n",
    "        t : 정답 레이블\n",
    "        Returns\n",
    "        -------\n",
    "        각 층의 기울기를 담은 사전(dictionary) 변수\n",
    "            grads['W1']、grads['W2']、... 각 층의 가중치\n",
    "            grads['b1']、grads['b2']、... 각 층의 편향\n",
    "        \"\"\"\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads\n",
    "        \n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "\n",
    "        for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):\n",
    "            self.layers[key].W = self.params['W' + str(i+1)]\n",
    "            self.layers[key].b = self.params['b' + str(i+1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN 시각화하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1번째 층의 가중치 시각화하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가중치의 형상은 (필터 30개, 채널 1개, 5 X 5 크기) 입니다. 필터의 크기가 5x5이며 1채널의 회색조 이미지로 시각화할 수 있다는 뜻입니다. 다음은 합성곱 계층 (1층째) 필터를 이미지로 나타낸 것입니다. 학습 전과 후의 가중치를 비교한 결과입니다. 학습 전 필터는 무작위로 초기화되고 있어 흑백의 정도에 규칙성이 없습니다. 한편, 학습을 마친 필터는 규칙성 있는 이미지가 되었습니다.  \n",
    "오른쪽과 같이 규칙성 있는 필터는 무엇을 보고 있는 걸까요? 그것은 에지(색상이 바뀐 경계선)와 블롭(국소적으로 덩어리진 영역) 등을 보고 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://media.vlpt.us/post-images/dscwinterstudy/b0d98ab0-4199-11ea-bed1-737062fffe57/fig-7-24.png\" width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 층 깊이에 따른 추출 정보 변화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "딥러닝 시각화에 관한 연구에 따르면, 계층이 깊어질수록 추출되는 정보(정확히는 강하게 반응하는 뉴런)은 더 추상화된다는 것을 알 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래의 네트워크 구조는 일반 사물 인식을 수행한 8층의 AlexNet입니다. 합성곱 계층과 풀링 계층을 여러 겹 쌓고, 마지막으로 완전연결 계층을 거쳐 결과를 출력하는 구조입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://media.vlpt.us/post-images/dscwinterstudy/c9235880-4199-11ea-9479-ad3ff669a3cd/fig-7-26.png\" width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "딥러닝의 흥미로운 점은 합성곱 계층을 여러 겹 쌓으면, 층이 깊어지면서 더 복잡하고 추상화된 정보가 추출된다는 점입니다. 처음 층은 단순한 에지에 반응하고, 이어서 텍스처에 반응하고, 더 복잡한 사물의 일부에 반응하도록 변화합니다. 즉, 층이 깊어지면서 뉴런이 반응하는 대상이 단순한 모양에서 고급 정보로 변화해갑니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 대표적인 CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지금까지 제안된 CNN 네트워크의 구성은 다양합니다. 이번 절에서는 그중에서도 특히 중요한 네트워크를 두 개 소개합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LeNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LeNet은 시그모이드 함수를 사용하지만, 현재는 ReLU를 사용\n",
    "- LeNet은 서브샘플링을 하여 중간 데이터의 크기를 줄이지만, 현재는 최대 풀링이 주류"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/850/1*AwJZkWLKabIicUPzSN6KCg.png\" width=70%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AlexNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 활성화 함수로 ReLU 이용\n",
    "- LRN(Local Response Normalization)이라는 국소적 정규화를 실시하는 계층 이용\n",
    "- 드롭아웃 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://t1.daumcdn.net/cfile/tistory/99FEB93C5C80B5192E\" width=60%>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "243px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
